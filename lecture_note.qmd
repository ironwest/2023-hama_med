---
title: "浜松医科大学講義"
format: html
editor_options: 
  chunk_output_type: console
toc: true
toc-location: right
---

```{r}
library(tidyverse)
```

## 資料で利用する記号

- $Y$ 目的変数
- $X_i(i=1,2,\cdots k)$ 説明変数
- $\beta_0$ 切片
- $\beta_i(i=1,2,\cdots k)$ 説明変数の係数

注:説明変数は資料を通して連続変数で、そのとりえる範囲を0-10とします。

Rのコードの中で#ではじまる部分はコメントと呼ばれる、注釈としての機能をもつ部分です。#R(キーワード):という記号で始めている部分はR言語に全くなじみのない方を対象にした説明です。説明がわからないという方は、キーワードを適宜検索するか、chatGPTなどで「R言語でキーワードについて解説してください」などと入力すればよいと思います。

tidyverseに関係する関数、tibble, mutate, filter, pipe(|>, %>%)などは講義中での解説は行いません。それ以外の関数については適宜解説を入れていきます。

## 重回帰分析のデータ生成

スライドで解説したものは少しややこしいのと、調整についてはDAGの話ででてくるので、ここでは、色々なデータを作るためのRの関数について紹介していきます。

### モデルその1：データ生成

まずは一番シンプルな単回帰分析です。

$$Y = \beta_0 + \beta_1 X_1 + \epsilon$$
任意の$\beta_0$, $\beta_1$ が結果として出てくるデータを作れるでしょうか？
（$X_1$は連続変数とします）

```{r}
#R 変数: 数字、データ、関数など、Rが扱えるもの(オブジェクト)に名前をつけることができます
#R 代入: 変数に対して 変数名 <- オブジェクト とすることで、後から変数名でオブジェクトを呼び出すことができます。
#R 関数：関数名()とすることで、色々な機能が実行されます。
#R 引数：関数名(引数名=オブジェクト)とすることで、関数の設定を変更して実施することができます。
#R ベクトル: 関数で作成できるオブジェクトの塊です。
#R パッケージ：Rにデフォルトでは入っていない拡張機能のことをパッケージといいます。
#R library(パッケージ名)：install.packages("パッケージ名")としてインストールできるパッケージを「Rでこれから使います」と宣言するための関数です。この資料で利用するパッケージは、tidyverse, gtsummaryの二つです。
#R tibble(列名=ベクトル):列名に対して、ベクトルを行の値とした表を作ることができます。この資料では、このtibbleで作成した表データに対して色々と操作を加えていきます。


#βの値を変数に入れて決めておく
beta0 <- 20
beta1 <- 3

#ついでにデータの個数もnという変数に入れて決めておく
n <- 1000

set.seed(12345) #ランダムさを固定する

#データの作成。X1はrunif()を利用して一様分布で適当に入れる
d <- tibble(x1 = runif(n,0,10))

d
```

runif()は、指定した範囲でランダムに数字を出させることができる関数です。

runif(生成したいデータの数、最少値、最大値)として、

次のようにベクトルをhist()でグラフ化してあげると、1から40まで、一様にデータが出現していますね。

```{r}
hist(runif(1000,0,40))
```

あとは、データフレームdの、x1列の数字を利用して、$\beta_0,\beta_1$を使って、目的変数を作成してあげればよいです。

```{r}
#R |>:パイプ関数と呼ばれるもので、表 |> 表を操作する関数 という書き方ができます。この関数を使うことで、表にどのような操作がどの順番で行われるかが分かりやすくなります。例えば表に操作１と操作２をこの順番で加える場合、関数で書くと、操作2(操作1(表))となりますが、パイプ関数を利用すると、表 |> 操作1() |>　操作2()とかけて、順番通りに 読むことができます。

#R mutate(新しい列名 = ベクトル): tibbleで作成した表に新しい列を追加する関数です。表の中に含まれている列名を使った計算もできます。beta0とbeta1は長さ1の数字ですが、x1が長さnの数字ベクトルなので、beta0 + beta1 * x1で、x1のそれぞれの値に対して、beta0とbeta1の値を使った計算が行われています。

d <- d |> 
  mutate(y = beta0 + beta1 * x1)
```

が、これだと、

```{r}
#R ggplot() + geom_point(aes(x = x1, y = y))：Rでグラフを行う場合、ggplotという描画の仕組みをよく利用します。この解説だけで本が１冊書けるので、とりあえずこの資料では出来上がるグラフに注目してください。

ggplot(d) + geom_point(aes(x = x1, y = y))
```

ただの直線になるので、$\epsilon$による残差（ズレ）を入れてあげます。
このとき、残差は正規分布するランダムな数であることが重回帰分析では必要なので、rnorm(生成するデータの数、平均、標準偏差)として設定してあげます。

次のコードで、epsilon列にrnormで作成した残差の数字が入っており、y列は、x1列とepsilon列の値を使って、事前に作成したbeta0とbeta1変数の数字を利用して計算していること、わかるでしょうか？

```{r}
d <- d |> 
  mutate(epsilon = rnorm(n,0,10)) |> 
  mutate(y = beta0 + beta1*x1 + epsilon)

d
```

この計算したx1とyを散布図としてあらわしてみると、だいぶ相関が強そうな結果となりました。

```{r}
ggplot(d) + geom_point(aes(x = x1, y = y))
```

ということで、これで、次のモデル

$$Y = \beta_0 + \beta_1 X_1 + \epsilon$$

で、$\beta_0=20; \quad \beta_1=3; \quad \epsilon \sim Norm(0,10)$とした場合のモデル、

$$Y = 20 + 3 X_1 + \epsilon$$

となるデータを生成することができました。

### モデルその1：分析する

それでは、生成したデータをもとに、重回帰分析（今回は説明変数が一つなので、単回帰分析ですが）を行ってみます。

重回帰分析はRではlm関数を利用します。lm関数の使い方は次のような形です。

```
lm(モデル式, データ)
```

ここで、モデル式の書き方を簡単に解説しておくと、

```
目的変数 ~ 説明変数1  +  説明変数2 + ... 
```

という書き方です。なので今回の場合は、

$$Y = \beta_0 + \beta_1 X_1 + \epsilon$$

というモデルを想定しているので、モデル式は

```
y ~ x1
```

となります。推定したい$\beta_0, \quad\beta_1$はモデル式に含まないことを確認しておいてください。それで、次のようにするとlm関数による回帰分析が実施できます。


```{r}
model <- lm(y~x1, d)

model
```

この結果、summary()関数でより詳細な結果を表示することも可能です。

```{r}
summary(model)
```

gtsummaryパッケージを利用するときれいな表を作成することも簡単にできます。tbl_regression関数にlmで作成したモデルを入れると描画できます。尚、intercept=TRUEと明示しないと切片は表示されないので注意が必要です。

```{r}
library(gtsummary)
tbl_regression(model,intercept = TRUE)
```

データは、Yを、切片が20でX1の係数が3を想定して作成しておりました。重回帰分析の結果と照らし合わせると、概ね一致した結果となっています。

$$Y = 20 + 3 X_1 + \epsilon$$

そのようにデータを作っているのだから当たり前ではありますが、この一連の流れを応用して、この後DAGのルールについて実際に確認していきます。









### モデルその2：演習

では、少し皆さんに取り組んでいただきたい課題として、次のモデル構造をもつデータを作成してみて、その結果が確かにその通りになっていることを確認していただけますか？(10分前後)


$$Y = 20 + X_1 + 2X_2 + 3X_3 + \epsilon$$
$\epsilon$は平均0、標準偏差10の正規分布。$X_1, X_2, X_3$はいずれも０から10の間の値をとりえて、一様分布から生成することとします。データの数は30個です。

ヒント：

```
n <- 100
d <- tibble(x1 = runif(n, 0, 10),...)

d <- d |> 
  mutate(epsilon = rnorm(n, 0, 10)) |> 
  mutate(y = 20 + ...)
  
model <- lm(y~...)

summary(model)
tbl_regression(model, ...)
```


答えの例：

```{r}
set.seed(12345) #これは、ランダムさを固定するためのもので皆さんには必要ありません
n <- 30
d <- tibble(x1 = runif(n, 0, 10),
            x2 = runif(n, 0, 10),
            x3 = runif(n, 0, 10))

d <- d |> 
  mutate(epsilon = rnorm(n, 0, 10)) |> 
  mutate(y = 20 + x1 + 2*x2 + 3*x3 + epsilon)

d
  
model <- lm(y~x1+x2+x3, data=d)

summary(model)
tbl_regression(model, intercept=TRUE)
```

どうでしょうか？今回、nの数を30個にしているので、出てきてほしい係数とはだいぶ違う数字になっていますが、これのnを今度は10000個にしてやり直してみるとどうなるでしょうか？

(コピペして実演：)

### モデルその2：95%信頼区間

統計的なアイデアの実験をRでできることの実例として、95%信頼区間について見てみます。ここでは少しややこしいプログラムを書くので、後日見ておいていただいてもよいかもしれません。

ここでやっていることは、スライド「95%信頼区間の実験」に記載してあることです。

やってみましょう。


```{r}
#真の係数と切片を変数にしておく、
beta0 <- 20
beta1 <- 1
beta2 <- 2
beta3 <- 3

# 1 n=30のデータ生成
d <- tibble(
  x1 = runif(30, 0, 10),
  x2 = runif(30, 0, 10),
  x3 = runif(30, 0, 10)
) |> 
  mutate(
    epsilon = rnorm(30, 0, 10),
    y = beta0 + beta1 * x1 + beta2 * x2 + beta3 *x3 + epsilon
  )

# 2 分析の実施
model <- lm(y~x1+x2+x3, d)


# 3 結果の保存
#95%信頼区間はstats::confint関数で計算できる。
#as_tibbleでconfintの結果をtibble形式にする
citable <- confint(model) |> as_tibble(rownames = "variable")

citable
```

上の1から3までの手順で、1回のデータ作成、分析、結果の保存を賄えました。

通常の研究の場合、データ作成（=データを集める）ステップは１回しかできないことが普通ですが、Rでデータ作成をしているため、好きなだけ今のプロセスを繰り返すことができます。関数にしてしまってもよいですが、今回はmap_dfr関数という中身を繰り返して、その結果が表の場合に、結果を「縦に積んでいく」処理ができる関数を利用して1000回繰り返します。
（この処理がややこしいので、わからない人はとりあえず次の処理を行うことで1000回繰り返されて、その結果が出てきているという理解で良いと思います。）


```{r}
result <- map_dfr(1:1000, ~{
  
  # 1 n=30のデータ生成
  d <- tibble(
    x1 = runif(30, 0, 10),
    x2 = runif(30, 0, 10),
    x3 = runif(30, 0, 10)
  ) |> 
    mutate(
      epsilon = rnorm(30, 0, 10),
      y = beta0 + beta1 * x1 + beta2 * x2 + beta3 *x3 + epsilon
    )
  
  # 2 分析の実施
  model <- lm(y~x1+x2+x3, d)
  
  
  # 3 結果の保存
  citable <- confint(model) |> 
    as_tibble(rownames = "variable")
  
  return(citable)
})
```

実施した結果、４行の結果が1000回、4000行のデータになっています。

```{r}
result　# 結果が毎回少しずつ違うのを確認してください。
```

それで、ぞれぞれの信頼区間はどれくらい真の値を含んでいるでしょうか？

```{r}
#列名を処理しやすいようにrenameで書き換えておく
result <- result |> 
  rename(lower = `2.5 %`,
         upper = `97.5 %`)

#variable列の値に応じて、真のβを95%CIが含むかどうかを調べる
result |> 
  mutate(is_contain = case_when(
    variable == "(Intercept)" ~ lower < beta0 & upper > beta0,
    variable == "x1" ~ lower < beta1 & upper > beta1,
    variable == "x2" ~ lower < beta2 & upper > beta2,
    variable == "x3" ~ lower < beta3 & upper > beta3
  )) |> 
  #variable列毎に、is_containの数を集計して、その割合を計算する
  group_by(variable) |> 
  summarise(n_contain = sum(is_contain),
            n_total   = n()) |> 
  mutate(p_contain = n_contain/n_total)
```

という結果になりました。95%信頼区間、同じ実験を何回も繰り返した場合に、『信頼区間「の」95%が、真の値を含む。』という結果になるの、おおよそご理解いただけたかと思います。（尚、ピッタリ95%ではないのは、試行回数をもっと増やせば95%に収束するはずです。

追記：1万回（それなりに時間がかかるため、当日は行いません）やった場合の結果は次のようでした：

```
 variable    n_contain n_total p_contain
  <chr>           <int>   <int>     <dbl>
1 (Intercept)      9508   10000     0.951
2 x1               9516   10000     0.952
3 x2               9492   10000     0.949
4 x3               9508   10000     0.951
```


## DAG

### Collidorの調整が見かけ上の相関関係を作る例

スライドで解説した、X1 -> X2 <- Y というCollidor関係となるデータを作成してみて、それの重回帰分析による調整がどのような結果を示すかを確認してみましょう。

```{r}
dag1 <- "digraph{
  X1 -> X2[label=0.5]
  Y  -> X2[label=0.7]
}"

DiagrammeR::grViz(dag1)
```

まず、矢印が入っていない変数を何らかのランダムな分布をもとに作成します。この講義では、原則一様分布と言われる、決めた範囲で等しく数がでてくる分布を利用します。なので、X1とYをrunifで作りましょう。この時点では、二つの変数にはなんの関係もありません。

```{r}
n <- 100
d <- tibble(x1 = runif(n, 0, 10),
            y  = runif(n, 0, 10))
```

```{r}
ggplot(d) + geom_point(aes(x = x1, y = y)) #関係ないですね？
```

それで、次に、X1とYから矢印が伸びている変数X2を作ります。X2は、X1とYから計算して（残差を除いて）求めることができるようにしておきます。


```{r}
d <- d |> 
  mutate(x2 = 0.5 * x1 +  0.7 * y + rnorm(n, 0, 1))
```

散布図にするとこんな感じで、X2はX1とY、両方と相関しています。

```{r}
g1 <- ggplot(d) + geom_point(aes(x = x1, y = x2))
g2 <- ggplot(d) + geom_point(aes(x = y, y = x2))
cowplot::plot_grid(g1,g2)
```


で、重回帰分析をしてみます。モデルを二つ作成して、それをlistに入れた上で、map関数を使ってtbl_regressionを適応して、その結果を横にモデルをくっつけられるtbl_mergeに渡してあります。

```{r}
model1 <- lm(y ~ x1, data = d)
model2 <- lm(y ~ x1 + x2, data = d)

list(model1, model2) |> 
  map(tbl_regression) |> 
  tbl_merge()
```

尚、参考までに次の二つのコードは同じ結果になります。

```
list(model1, model2) |> map(tbl_regression) |> tbl_merge()

tbl_merge(list(tbl_regression(model1), tbl_regression(model2)))

```

いかがでしょうか？x1とyにはなんの関係もないというのが正しいはずです。単純に回帰分析したTable1の結果は正しい関係を表現できていますが、x2で調整することで、x1の係数が有意となってしまい、誤った結論となっています。

これは、図で見ると分かりやすいので図でも確認しておきます。x2を10分位に区切ってカテゴリー変数としてしまい、色分けしてプロットしてみましょう。

```{r}
d <- d |> 
  mutate(x2cat = Hmisc::cut2(x2, g = 10))

ggplot(d) + 
  geom_point(aes(x = x1, y = y, color = x2cat)) +
  facet_wrap(~x2cat)
```

いかがでしょうか？x2のカテゴリー内だけでみてみると、負の相関があるように見えます。これは、数学的にも当たり前で、このデータは次のようなモデルで作成されています。

$$X_2 = \alpha_0 + \alpha_1 X_1 + \alpha_2 Y \cdots1$$
それなのに、実際に実施した重回帰分析は、

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 \cdots2$$

です。1の式を変形すると、


$$\alpha_0 + \alpha_1 X_1 + \alpha_2 Y = X_2$$

$$\alpha_2 Y =  - \alpha_0 - \alpha_1 X_1 +  X_2$$

$$Y = - \frac{\alpha_0}{\alpha_2} - \frac{\alpha_1}{\alpha_2} X_1 + \frac{1}{\alpha_2} X_2  = \beta_0 + \beta_1 X_1 + \beta_2 X_2$$
結果、$\beta_1=-\frac{\alpha_1}{\alpha_2}$となり、YとX1との間に相関関係が生じているように見える結果が生じてしまいました。

ということで、Collidorを調整すると、本来ないはずの相関関係が生じてしまうということ、ご納得いただけましたでしょうか。


### 課題

1 中間変数を調整すれば、パスが閉じる

```{r}
DiagrammeR::grViz("
                  digraph{
                  E -> X[label=3]; 
                  X -> Y[label=5]}
                  ")
```

課題：データの生成を行ってみてください。(5分)

```
#データの作成
n <- 100
d <- tibble(e = runif(n,0,10)) |> 
  mutate(??? = 10 + ??? + rnorm(n,0,10)) |> 
  mutate(y = 5  + ??? + rnorm(n,0,10))
```


```{r}
#データの作成
n <- 100
d <- tibble(e = runif(n,0,10)) |> 
  mutate(x = 10 + 3*e + rnorm(n,0,10)) |> 
  mutate(y = 5  + 5*x + rnorm(n,0,10))
```

課題：次にY~E, Y~X, Y ~ E + Xの3モデルを作って回帰分析を実施してみて、その結果がDAGのパスで想定した結果と相違ないことを確認しください。


```
#モデルの作成
model1 <- ???
model2 <- ???
model3 <- ???

#結果の図示
list(model1, model2, model3) |> 
  map(tbl_regression) |> 
  tbl_merge()
```


```{r}
#モデルの作成
model1 <- lm(y~e, d)
model2 <- lm(y~x, d)
model3 <- lm(y~e+x,d)

#結果の図示
list(model1, model2, model3) |> 
  map(tbl_regression) |> 
  tbl_merge()
```

Eだけで分析するとYとの相関があるのに対して、パスの途中のXを入れて分析すると、Eの係数は0になり、Xの係数は5とデータの作成で利用した係数通りの結果となっています。

2 交絡因子を調整すれば、パスが閉じる

```{r}
DiagrammeR::grViz("
                  digraph{
                  X -> E[label=4]; 
                  X -> Y[label=7]}
                  ")
```


課題：データの生成を行ってみてください。(5分)

```
#データの作成
n <- 100
d <- tibble(x = runif(n,0,10)) |> 
  mutate(e = 10 + ??? + rnorm(n,0,10)) |> 
  mutate(y = 5  + ??? + rnorm(n,0,10))
```


```{r}
#データの作成
n <- 100
d <- tibble(x = runif(n,0,10)) |> 
  mutate(e = 10 + 4*x + rnorm(n,0,10)) |> 
  mutate(y = 5  + 7*x + rnorm(n,0,10))
```

課題：次にY~E, Y~X, Y ~ E + Xの3モデルを作って回帰分析を実施してみて、その結果がDAGのパスで想定した結果と相違ないことを確認しください。（５分)

```
#モデルの作成
model1 <- ???
model2 <- ???
model3 <- ???

#結果の図示
list(model1, model2, model3) |> 
  map(tbl_regression) |> 
  tbl_merge()
```



```{r}
#モデルの作成
model1 <- lm(y~e, d)
model2 <- lm(y~x, d)
model3 <- lm(y~e+x,d)

#結果の図示
list(model1, model2, model3) |> 
  map(tbl_regression) |> 
  tbl_merge()
```


Eだけで分析すると、係数は1程度。Xで調整すると、Eの係数はほぼ0に。CollidorであるXを調整しないと、本来は関係のないEとYの関係が正しく推定できません。


3 Collidorから出ているパスのどこかを調整すると、パスが開く

```{r}
DiagrammeR::grViz("
                  digraph{
                  E -> X[label=5]; 
                  X -> Z[label=6]; 
                  Y -> X[label=4];
                  Z[shape=rectangle]}
                  ")
```



課題：データの生成を行ってみてください。(7分)

```
#データの作成
n <- 100
d <- tibble(
    ??? = runif(n,0,10),
    ??? = runif(n,0,10)
  ) |>
  mutate(x = 10 + ??? + rnorm(n,0,5),
         z = 5  + ??? + rnorm(n,0,5))
```


```{r}
#データの作成
n <- 100
d <- tibble(
    e = runif(n,0,10),
    y = runif(n,0,10)
  ) |>
  mutate(x = 10 + 5*e + 4*y + rnorm(n,0,5),
         z = 5  + 6*x + rnorm(n,0,5))
```

課題：次にY~E, Y~Z, Y ~ Z、Y ~ E+X, Y~E+Zの3モデルを作って回帰分析を実施してみて、その結果がDAGのパスで想定した結果と相違ないことを確認してください。（５分)


```{r}
#モデルの作成
model1 <- lm(y~e, d)
model2 <- lm(y~z, d)
model3 <- lm(y~x, d)
model4 <- lm(y~e+x, d)
model5 <- lm(y~e+z, d)

#結果の図示
list(model1, model2, model3, model4, model5) |> 
  map(tbl_regression) |> 
  tbl_merge()
```

eだけで分析すると、係数は0で、Zあるいはxと同時に分析すると、有意な結果となっており、本来関係がないはずのEとYにCollidorであるX、あるいは、Collidorから出ているパスのZを調整することで関係があるという結果になっている

以上、調性を行うことで、3パターンのパスが閉じる/開く場合について、実際にデータを作成して確認してみました。


## バックドアパス

それでは、ここでは複雑な構造をもつデータに対してバックドアパスが閉じられると、推定値を正しく求められることを見ていきましょう。

まず、スライドで示した構造をもつデータを作成します。ここで、各エッジについている数字は、係数を表します。

```{r}
set.seed(12345)

DiagrammeR::grViz("
digraph{
  E  -> O [label=2];
  E  -> X1[label=3];
  E  -> X2[label=4];
  X2 -> X1[label=5];
  X4 -> E [label=6];
  X4 -> O [label=7];
  X3 -> X2[label=8];
  O  -> X1[label=9];
}
                  ")
```

係数を利用してデータを作成していきます。ここでのデータの作成方法は、

 * 何も線が入らない場合は0-10の一様分布からランダムにデータが発生
 * それぞれの係数をかけたもので次のデータを生成
 
 とします。なので、例えば、変数$X_1$のデータは、$X_1 = 3E+5X_2$として生成されるという形です。
 
 この構造を持つデータ、作成できますでしょうか？
 
 課題：上の構造を持つデータを作成してみてください。(10分)



 解答例：
 
```{r}
set.seed(12345)
n <- 1000
d <- tibble(
  x3 = runif(n,0,10),
  x4 = runif(n,0,10)
)

d <- d |> 
  mutate(
    e  = 6*x4 + rnorm(n, 0, 10),
    o  = 7*x4 + 2*e + rnorm(n, 0, 10), #ここのeの前の2という数字を重回帰分析でもとめたい
    x2 = 4*e  + 8*x3 + rnorm(n, 0, 10),
    x1 = 3*e  + 5*x2 + 9*o +rnorm(n, 0, 10)
  )
```

データを見てみると、
 
```{r}
ggplot(d) + geom_point(aes(x = e, y = o))
```

EとOに強い相関があります。単純に重回帰分析をしてみると、

```{r}
lm(o~e,data=d)
```

Eの係数は実際に計算で利用した2とは少し違う値になっています。ここで、スライドであるように、X4で調性するとバックドアパスが全て閉じられるため、正しい推定値である2がでてくるはずです。

```{r}
lm(o~e+x4,data=d)
```

でてきました。それでは、もし変数が沢山あるからといって全部で調性してしまうとどうなるでしょうか？

```{r}
lm(o~e+x1+x2+x3+x4, d)
```

Eの値は２と全然違う値になりました。これ、CollidorであるX1での調性に起因して生じている現象です。実際に色々なパターンでの調性で比較してみると、

```{r}
mod1 <- lm(o~e+x1, d)
mod2 <- lm(o~e+x2, d)
mod3 <- lm(o~e+x3, d)
mod4 <- lm(o~e+x4, d)

mod5 <- lm(o~e+x1 + x2, d)
mod6 <- lm(o~e+x1 + x3, d)
mod7 <- lm(o~e+x1 + x4, d)

mod8 <- lm(o~e+x2 + x4, d)
mod9 <- lm(o~e+x3 + x4, d)

list(mod1,mod2,mod3,mod4,mod5,mod6,mod7,mod8,mod9) |> 
  map(tbl_regression) |>
  tbl_merge()
```

データの構造として出てきてほしい2という係数の値は、

* X4で調性しないと2はでてこない
* ただし、X1で調性するとX4で調性していても2はでてこない

という結果になっており、DAGのルール通りの結果となっています。

## 喫煙、低体重出生、死亡のパラドックス

つぎのような構造をもつデータを作って、論文で紹介されていたグラフ（のようなデータ）を再現できるか試みてみます。

```{r}
DiagrammeR::grViz("
  digraph{
    S -> L;
    S -> M;
    U -> L;
    U -> M;
    S[label='喫煙'];
    U[label='他の要因'];
    M[label='死亡'];
    L[label='出生体重'];
  }                  
")
```


ここで他の要因を確率0.5%で発症する先天性疾患のイメージでデータを作っていきます。

sample関数は「くじ引き」をするような関数で、
sample(くじの内容のベクトル、引く数、一度引いたくじを戻すかどうが、それぞれのくじの内容の出現確率)という記載を行います

例えば、赤、白、青のくじが一つずつ入った箱からくじを１本引いて、結果を記録して、そのくじを戻す操作を10回行う場合は、

```{r}
sample(
  x = c("赤","白","青"), 
  size=10, 
  replace=TRUE, 
  prob = c(1/3,1/3,1/3)　#等確率であれば、probは必須ではありません。
)
```

となります。

他にも、

例えば、赤が30個、白が20個、青が10個のくじが一つずつ入った箱からくじを１本引いて、結果を記録して、そのくじを戻さない操作を10回行う場合は、

```{r}
sample(
  x = c(rep("赤",30),
        rep("白",20),
        rep("青",10)), 
  size=10, 
  replace=FALSE
)
```

となります。

また死亡の有無は、２値変数であるため、$\beta_0 + \beta_1X_1 +\cdots$で計算できる連続数を確率に変換できるロジスティック関数を利用して、50%以上で死亡となるようにしてあります。

ロジスティック関数は、

$$p = \frac{1}{1+exp(-X)}$$

という複雑に見える関数ですが、Xとpのグラフを書いてみると、


```{r}
logisticfrac <- function(x){
  1/(1+exp(-x))
}

ggplot() +
  geom_function(fun=logisticfrac, xlim = c(-10,10))
```

こんなグラフになっており、xの値がマイナス無限で0、プラス無限で1となる関数です。この関数の形、xの値から確率が作られると考える場合に非常に都合がよいため、ロジスティック回帰分析の基本となる部分で利用されています。

xの部分に、$\beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k$を入れると、係数を使って変数との掛け算を足し合わせた値を確率へと変換することができます。

以上の知識を利用して、パラドックスを再現するデータを生成してみましょう。

```{r}
set.seed(12345)
n <- 10000000
smoking <- sample(c(0,1),n,TRUE,prob=c(0.8,0.2)) #喫煙は20%の確率
otherdx <- sample(c(0,1),n,TRUE,prob=c(1-0.005, 0.005)) #遺伝性疾患は0.1%の確率
  
bw <- 3500 + rnorm(n,0,700) - 500*smoking -1000*otherdx #体重を喫煙、遺伝性疾患も踏まえて決める
death_linear <- -10 + 3*smoking + 10*otherdx + rnorm(n,0,3) 
#死亡を体重は関係なく、喫煙と遺伝性疾患のみで数字としてきめる
death_prob <- 1/(1+exp(-death_linear))
#死亡の数字をロジスティック関数を利用して確率に変換
death <- if_else(death_prob>=0.5,1,0) #確率が50%以上で死亡とする
  
#表にする
d <- tibble(
  smoking = smoking,
  otherdx = otherdx,
  bw = bw,
  death_prob = death_prob,
  death = death
)

#グラフにする場合に体重が細かい数値だと不都合なので、00の単位にまるめる
#800以下、5000より大きい体重はそれぞれ800と5000にしておく
d2 <- d |> 
  mutate(bw = round(bw,-2)) |> #BWを100単位にする
  mutate(bw = if_else(bw <= 800, 800, bw)) |>  # 800以下の体重を800に
  mutate(bw = if_else(bw > 5000, 5000, bw))# >5000を5000に

#作成したデータをもとにグラフを作成する
gdat <- d2 |>
  count(smoking, bw, death) |> 
  group_by(smoking,bw) |>
  mutate(tot = sum(n), p = n/tot) |> 
  filter(death == 1) |> 
  mutate(smoking = as.factor(smoking))
  
ggplot(gdat) + 
  geom_line(aes(x = bw, y = p, color = smoking, group=smoking)) +
  geom_point(aes(x = bw, y = p, color = smoking)) +
  theme_classic()
```

論文にあるようなグラフが生成できました。なお、このデータ、ちゃんと未測定の交絡要因で層別化してみてあげると、

```{r}
gdat2 <- d2 |> 
  count(smoking, otherdx, bw, death) |> 
  group_by(smoking, otherdx, bw) |>
  arrange(bw) |> 
  mutate(tot= sum(n),
         p  = n/tot) |> 
  filter(death==1) |> 
  mutate(smoking = factor(smoking, 
                          levels=0:1, labels=c("Smo-","Smo+")),
         otherdx = factor(otherdx,
                          levels=0:1, labels=c("Oth-","Oth+")))

ggplot(gdat2) + 
  geom_line(aes(x = bw, y = p, color = smoking, group=smoking)) +
  geom_point(aes(x = bw, y = p, color = smoking)) +
  theme_classic() +
  facet_wrap(~otherdx, scales = "free_y")
```

その他未測定の交絡がある場合もない場合も、喫煙はちゃんと？死亡率の上昇につながっている一方、最初のグラフで「見えた」体重と死亡率との相関ははっきりとしていません。（5000近辺で値が揺れているのはサンプルサイズの問題に思います）

このデータの生成、かなり恣意的な数値で「そう見える」ように値をいじって作成したデータですが、今回のようなパラドックスを考える場合にDAGを使って仮説を考えられることは有用ではないかと思います。

二値変数を用いる場合は、ロジスティック関数の中の数値を考えなければならないのでちょっとややこしいですが、なれると、このようなデータを作成することもできるようになります。





